{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "from typing import Dict, Any\n",
    "from pathlib import Path\n",
    "import tempfile\n",
    "\n",
    "from allennlp.common import Params\n",
    "import target_extraction\n",
    "from target_extraction.allen import AllenNLPModel\n",
    "import lstm\n",
    "\n",
    "import config\n",
    "from target_extraction.analysis.sentiment_metrics import get_labels\n",
    "from target_extraction.data_types import TargetTextCollection\n",
    "import json\n",
    "\n",
    "def predictions(model_name, split_name, dataset_name, preidction_collection, save_dir):\n",
    "    result_fp = save_dir / f'{dataset_name} {split_name}.json'\n",
    "    labels = get_labels(preidction_collection, 'target_sentiments', 'predicted_sentiment')\n",
    "    with result_fp.open('w+') as result_file:\n",
    "        json.dump(labels, result_file)\n",
    "\n",
    "def add_word_vector(word_vector_name: str, params_dict: Dict[str, any], \n",
    "                    model_name: str, file_path: Path) -> Path:\n",
    "    temp_params_dict = copy.deepcopy(params_dict)\n",
    "    word_embedding_fp = None\n",
    "    dimension = 0\n",
    "    if word_vector_name == 'GloVe 300':\n",
    "        word_embedding_fp = config.WORD_EMBEDDING_DIR / 'glove.840B.300d.txt'\n",
    "        dimension = 300\n",
    "    else:\n",
    "        raise ValueError(f'word_vector_name {word_vector_name} is not one of accepted names.')\n",
    "    assert word_embedding_fp is not None\n",
    "    word_embedding_fp = str(word_embedding_fp)\n",
    "    if model_name in ['TDLSTM', 'TCLSTM']:\n",
    "        temp_params_dict['model']['context_field_embedder']['token_embedders']['tokens']['embedding_dim'] = dimension\n",
    "        temp_params_dict['model']['context_field_embedder']['token_embedders']['tokens']['pretrained_file'] = word_embedding_fp\n",
    "    elif model_name == 'LSTM':\n",
    "        temp_params_dict['model']['embedder']['token_embedders']['tokens']['embedding_dim'] = dimension\n",
    "        temp_params_dict['model']['embedder']['token_embedders']['tokens']['pretrained_file'] = word_embedding_fp\n",
    "    else:\n",
    "        raise ValueError(f'The model name {model_name} is not a recognised model name')\n",
    "    if model_name == 'TDLSTM':\n",
    "        temp_params_dict['model']['left_text_encoder']['hidden_size'] = dimension\n",
    "        temp_params_dict['model']['left_text_encoder']['input_size'] = dimension\n",
    "        temp_params_dict['model']['right_text_encoder']['hidden_size'] = dimension\n",
    "        temp_params_dict['model']['right_text_encoder']['input_size'] = dimension\n",
    "    elif model_name == 'TCLSTM':\n",
    "        temp_params_dict['model']['left_text_encoder']['hidden_size'] = dimension\n",
    "        temp_params_dict['model']['left_text_encoder']['input_size'] = dimension * 2\n",
    "        temp_params_dict['model']['right_text_encoder']['hidden_size'] = dimension\n",
    "        temp_params_dict['model']['right_text_encoder']['input_size'] = dimension * 2\n",
    "        temp_params_dict['model']['target_encoder']['embedding_dim'] = dimension\n",
    "    elif model_name == 'LSTM':\n",
    "        temp_params_dict['model']['encoder']['hidden_size'] = dimension\n",
    "        temp_params_dict['model']['encoder']['input_size'] = dimension\n",
    "    else:\n",
    "        raise ValueError(f'The model name {model_name} is not a recognised model name')\n",
    "    #print(temp_params_dict)\n",
    "    return Params(temp_params_dict).to_file(file_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mass evaluation predictions for the LSTM based methods\n",
    "\n",
    "This is the notebook that creates the mass evaluation predictions for the [Tang et al. 2016](https://www.aclweb.org/anthology/C16-1311.pdf) LSTM based methods. The cell below creates the predictions for all datasets in their normal size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "# datasets\n",
    "patience_values = [\"5\", \"10\"]\n",
    "dataset_names = ['YouTuBean', 'Dong', 'Election', 'Laptop', 'Mitchell', 'Restaurant']\n",
    "for patience_value in patience_values:\n",
    "    for dataset_name in dataset_names:\n",
    "        train_dataset = TargetTextCollection.load_json(config.neural_dataset_dir / f'{dataset_name} train.json')\n",
    "        validation_dataset = TargetTextCollection.load_json(config.neural_dataset_dir / f'{dataset_name} validation.json')\n",
    "        test_dataset = TargetTextCollection.load_json(config.neural_dataset_dir / f'{dataset_name} test.json')\n",
    "        for model_name in ['LSTM', 'TDLSTM', 'TCLSTM']:\n",
    "            result_dir = config.RESULTS_DIR / 'Mass Evaluation' / f\"patience {patience_value}\" / f\"{model_name}\"\n",
    "            result_dir.mkdir(parents=True, exist_ok=True)\n",
    "            validation_result_fp = result_dir / f'{dataset_name} validation.json'\n",
    "            test_result_fp = result_dir / f'{dataset_name} test.json'\n",
    "            if validation_result_fp.exists() and test_result_fp.exists():\n",
    "                continue\n",
    "\n",
    "            val_copy = TargetTextCollection(copy.deepcopy(list(validation_dataset.values())))\n",
    "            test_copy = TargetTextCollection(copy.deepcopy(list(test_dataset.values())))\n",
    "            base_config = config.MODEL_CONFIG_DIR / f\"patience {patience_value}\" / f\"{model_name}.jsonnet\"\n",
    "            predictor = 'target-sentiment'\n",
    "            for i in range(6):\n",
    "                with tempfile.NamedTemporaryFile() as temp_file:\n",
    "                    temp_file_path = Path(temp_file.name)\n",
    "                    add_word_vector('GloVe 300', Params.from_file(base_config).as_dict(), \n",
    "                                    model_name, temp_file_path)\n",
    "                    a_model = AllenNLPModel(model_name, temp_file_path, predictor, save_dir=None)\n",
    "                    a_model.fit(train_data=train_dataset, val_data=validation_dataset, test_data=test_dataset)\n",
    "                    a_model.predict_into_collection(val_copy, {'sentiments': 'predicted_sentiment'}, batch_size=32)\n",
    "                    a_model.predict_into_collection(test_copy, {'sentiments': 'predicted_sentiment'}, batch_size=32)\n",
    "            predictions(model_name, 'validation', dataset_name, val_copy, result_dir)\n",
    "            predictions(model_name, 'test', dataset_name, test_copy, result_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the cell below creates the prediction for all datasets trained on the much smaller sized training dataset. This smaller sized training set is the same size as the YouTuBean training dataset size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "# datasets\n",
    "dataset_names = ['Dong', 'Election', 'Laptop', 'Mitchell', 'Restaurant']\n",
    "patience_value = \"10\"\n",
    "for dataset_name in dataset_names:\n",
    "    train_dataset = TargetTextCollection.load_json(config.neural_small_dataset_dir / f'{dataset_name} train.json')\n",
    "    validation_dataset = TargetTextCollection.load_json(config.neural_small_dataset_dir / f'{dataset_name} validation.json')\n",
    "    test_dataset = TargetTextCollection.load_json(config.neural_dataset_dir / f'{dataset_name} test.json')\n",
    "    for model_name in ['LSTM', 'TDLSTM', 'TCLSTM']:\n",
    "        result_dir = config.RESULTS_DIR / 'Mass Evaluation Small Dataset' / f\"patience {patience_value}\" / f\"{model_name}\"\n",
    "        result_dir.mkdir(parents=True, exist_ok=True)\n",
    "        validation_result_fp = result_dir / f'{dataset_name} validation.json'\n",
    "        test_result_fp = result_dir / f'{dataset_name} test.json'\n",
    "        if validation_result_fp.exists() and test_result_fp.exists():\n",
    "            continue\n",
    "\n",
    "        val_copy = TargetTextCollection(copy.deepcopy(list(validation_dataset.values())))\n",
    "        test_copy = TargetTextCollection(copy.deepcopy(list(test_dataset.values())))\n",
    "        base_config = config.MODEL_CONFIG_DIR / f\"patience {patience_value}\" / f\"{model_name}.jsonnet\"\n",
    "        predictor = 'target-sentiment'\n",
    "        for i in range(6):\n",
    "            with tempfile.NamedTemporaryFile() as temp_file:\n",
    "                temp_file_path = Path(temp_file.name)\n",
    "                add_word_vector('GloVe 300', Params.from_file(base_config).as_dict(), \n",
    "                                model_name, temp_file_path)\n",
    "                a_model = AllenNLPModel(model_name, temp_file_path, predictor, save_dir=None)\n",
    "                a_model.fit(train_data=train_dataset, val_data=validation_dataset, test_data=test_dataset)\n",
    "                a_model.predict_into_collection(val_copy, {'sentiments': 'predicted_sentiment'}, batch_size=32)\n",
    "                a_model.predict_into_collection(test_copy, {'sentiments': 'predicted_sentiment'}, batch_size=32)\n",
    "        predictions(model_name, 'validation', dataset_name, val_copy, result_dir)\n",
    "        predictions(model_name, 'test', dataset_name, test_copy, result_dir)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7-candidate"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python36764bitf868c7e0d1e547669dece7a4caca1993",
   "display_name": "Python 3.6.7 64-bit"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}