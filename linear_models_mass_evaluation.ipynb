{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": "Using TensorFlow backend.\n"
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "from typing import Callable, Dict, List, Any, Tuple\n",
    "from pathlib import Path\n",
    "import math\n",
    "import json\n",
    "\n",
    "from bella import tokenisers\n",
    "from bella import lexicons\n",
    "from bella import parsers\n",
    "from bella.models.tdparse import TDParse, TDParsePlus, TDParseMinus\n",
    "from bella.models.target import TargetDep, TargetDepPlus\n",
    "from bella.data_types import TargetCollection, Target\n",
    "from bella.models.base import SKLearnModel\n",
    "from bella.word_vectors import GloveCommonCrawl\n",
    "from bella.dependency_parsers import tweebo\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "import config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_C_cross_val_predictions(datasets: List[TargetCollection],\n",
    "                                 models: List[SKLearnModel],\n",
    "                                 metric: str = 'accuracy') -> Dict[str, TargetCollection]:\n",
    "    '''\n",
    "    Returns a dictionary of `{dataset_name} {model_name} {fold_number}` as \n",
    "    keys and TargetCollection as values where the TargetCollections store the \n",
    "    test data and the predictions made for that cross validation fold on the \n",
    "    dataset for that specific model.\n",
    "    '''\n",
    "    collection_name_index:  Dict[str, int] = {}\n",
    "    for dataset in datasets:\n",
    "        dataset_name = dataset.name\n",
    "        for model in models:\n",
    "            model_name = model.name()\n",
    "            model_dir = config.RESULTS_DIR / 'C Value' / model_name\n",
    "            all_scores = []\n",
    "            for fold_number in range(5):\n",
    "                fold_name = f'{dataset_name} {model_name} {fold_number}'\n",
    "                label_file_name = f'{dataset_name} {fold_number} labels.npy'\n",
    "                label_fp = str((model_dir / label_file_name).resolve())\n",
    "                test_collection = np.load(label_fp, allow_pickle=True)\n",
    "                test_collection = TargetCollection([Target(**data) \n",
    "                                                    for data in test_collection])\n",
    "                if metric == 'accuracy':\n",
    "                    scores = test_collection.dataset_metric_scores(accuracy_score)\n",
    "                else:\n",
    "                    scores = test_collection.dataset_metric_scores(f1_score, average='macro')\n",
    "                all_scores.append(np.expand_dims(scores, 1))\n",
    "            all_scores = np.concatenate(all_scores, 1)\n",
    "            mean_scores = all_scores.mean(axis=1)\n",
    "            best_mean_index = np.argmax(mean_scores)\n",
    "            collection_name_index[f'{dataset_name} {model_name}'] = best_mean_index\n",
    "    return collection_name_index\n",
    "\n",
    "def get_specific_params(best_param_dicts: List[Dict[str, int]],\n",
    "                        param_names: List[str],\n",
    "                        param_values: List[List[Any]]\n",
    "                        ) -> Dict[str, Dict[str, Dict[str, Any]]]:\n",
    "    '''\n",
    "    Returns a dictionary that can be used as the specific_params argument in the\n",
    "    default_params function. The return finds the best parameter value for a \n",
    "    model on a specific dataset and returns this as a dictionary of dataset names\n",
    "    which contains a dictionary of model names which contains a dictionary of \n",
    "    parameter names and their optimal/best parameter value for the model on \n",
    "    the specific dataset.\n",
    "    \n",
    "    At the moment all dataset names can be only one word long or else \n",
    "    this function will fail.\n",
    "    \n",
    "    :param best_param_dicts: A list of the return of the best_param_index \n",
    "                             function for each parameter name\n",
    "    :param param_name: The list of parameter names\n",
    "    :param param_values: A List of possible parameter values for each\n",
    "                         parameter name.\n",
    "    :returns: A dictionary of model names which contains a dictionary of \n",
    "              parameter names and their optimal/best parameter value for \n",
    "              the model on the specific dataset.\n",
    "    '''\n",
    "    if (len(best_param_dicts) != len(param_names) and\n",
    "        len(param_names) != len(param_values)):\n",
    "        raise ValueError('The length of all three arguments must be the same')\n",
    "    \n",
    "    optimal_param = defaultdict(lambda: defaultdict(lambda: dict()))\n",
    "    for index, best_param_dict in enumerate(best_param_dicts):\n",
    "        for dataset_model_name, best_index in best_param_dict.items():\n",
    "            split_names = dataset_model_name.split()\n",
    "            dataset_name = split_names[0]\n",
    "            model_name = ' '.join(split_names[1:])\n",
    "            best_param_value = param_values[index][best_index]\n",
    "            param_name = param_names[index]\n",
    "            optimal_param[dataset_name][model_name][param_name] = best_param_value\n",
    "    return optimal_param\n",
    "\n",
    "def load_convert_results(result_data: Tuple[List[str], List[List[str]]],\n",
    "                         mapper: Dict[str, int]) -> Tuple[np.ndarray, np.ndarray]:\n",
    "    '''\n",
    "    Returns a tuple of true values and predictions where the true values is\n",
    "    of shape (n_sample) and the predictions are of shape (n_samples, num_runs)\n",
    "    '''\n",
    "    true_values, run_prediction_values = result_data\n",
    "    true_values = np.array([mapper[value] for value in true_values])\n",
    "    run_prediction_values = [[mapper[prediction] for prediction in predictions] \n",
    "                              for predictions in run_prediction_values]\n",
    "    return true_values, np.array(run_prediction_values).T\n",
    "\n",
    "def get_score(true_values: np.ndarray, run_predictions: np.ndarray, \n",
    "              metric, **metric_kwargs) -> np.ndarray:\n",
    "    num_runs = run_predictions.shape[1]\n",
    "    scores = []\n",
    "    for run in range(num_runs):\n",
    "        scores.append(metric(true_values, run_predictions[:, run], \n",
    "                      **metric_kwargs))\n",
    "    return np.array(scores)\n",
    "\n",
    "label_mapper = {'positive': 2, 'negative': 1, 'neutral': 3}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mass Evaluation of [Vo et al. 2015](https://ijcai.org/Proceedings/15/Papers/194.pdf) and [Wang et al. 2017](https://www.aclweb.org/anthology/E17-1046.pdf) models\n",
    "\n",
    "In this notebook we train and evaluate the two best performing models from [Vo et al. 2015](https://ijcai.org/Proceedings/15/Papers/194.pdf) and [Wang et al. 2017](https://www.aclweb.org/anthology/E17-1046.pdf) which are:\n",
    "\n",
    "1. Target Dependent (Vo)\n",
    "2. Target Dependent Plus (Vo)\n",
    "3. TDParse (Wang) \n",
    "4. TDParse Plus (Wang)\n",
    "\n",
    "Of which the methods with `Plus` in their name use sentiment lexicons. Based on the development results from [./large_scale_feature_settings.ipynb](./large_scale_feature_settings.ipynb) we use the best performing `C value` for each method on each dataset. We also scale the features using MinMax scaling as that was also found to be generally significantly better on all methods and datasets. Finally for the sentiment lexicons as none of the lexicons that contain the Hu and Liu (HL) were ever significantly worse than the best performing sentiment lexicon for each method on each dataset we use a combination of all lexcions the relevant methods on all datasets. The use of all sentiment lexicons rather than just the best performing was due to the three lexcions originally coming from different types MPQA (news), HL (reviews), and NRC (Twitter/social media). The best C value was chosen based on the accuracy metric and not the F1. Further we note that for the F1 metric that using all three lexicons was significantly worse for 1 fold out of the 5 folds for two datasets but generally using all three lexicons is significantly no worse than the best performing.\n",
    "\n",
    "The tokeniser used is the Spacy tokeniser due to it's speed and wide use in the NLP field. We use the [Tweebo dependency parser](http://www.cs.cmu.edu/~nasmith/papers/kong+schneider+swayamdipta+bhatia+dyer+smith.emnlp14.pdf) as it is the only parser that creates multiple roots for a given sentence. Finally we lower case all words and use the [GloVe 300 dimension 840 billion token word embedding](https://nlp.stanford.edu/projects/glove/).\n",
    "\n",
    "Below we load all of the six datasets, lexicons, and word embedding:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Loading glove 300d 840b common crawl from file\n"
    }
   ],
   "source": [
    "# Load the sentiment lexicons\n",
    "subset_cats = {'positive', 'negative'}\n",
    "mpqa = lexicons.Mpqa(config.MPQA, \n",
    "                     subset_cats=subset_cats, lower=True)\n",
    "hu_liu = lexicons.HuLiu(config.HL, \n",
    "                        subset_cats=subset_cats, lower=True)\n",
    "nrc = lexicons.NRC(config.NRC, subset_cats=subset_cats, lower=True)\n",
    "mpqa_huliu = lexicons.Lexicon.combine_lexicons(mpqa, hu_liu)\n",
    "mpqa_nrc = lexicons.Lexicon.combine_lexicons(mpqa, nrc)\n",
    "huliu_nrc = lexicons.Lexicon.combine_lexicons(hu_liu, nrc)\n",
    "all_lexicons = lexicons.Lexicon.combine_lexicons(mpqa_huliu, nrc)\n",
    "\n",
    "tokeniser = tokenisers.spacy_tokeniser\n",
    "default_random_state = 42\n",
    "\n",
    "# Results directorys\n",
    "result_dir = config.RESULTS_DIR / 'Mass Evaluation'\n",
    "result_dir.mkdir(exist_ok=True, parents=True)\n",
    "small_result_dir = config.RESULTS_DIR / 'Mass Evaluation Small Dataset'\n",
    "small_result_dir.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "# Image directory\n",
    "image_dir = config.IMAGES_DIR\n",
    "image_dir = image_dir / 'Mass Evaluation'\n",
    "\n",
    "# Datasets \n",
    "dong_train = parsers.dong(config.DONG_TRAIN, name='Dong Train')\n",
    "dong_test = parsers.dong(config.DONG_TEST, name='Dong')\n",
    "laptop_train = parsers.semeval_14(config.laptop_train, name='Laptop Train')\n",
    "laptop_test = parsers.semeval_14(config.laptop_test, name='Laptop')\n",
    "restaurant_train = parsers.semeval_14(config.restaurant_train, name='Restaurant Train')\n",
    "restaurant_test = parsers.semeval_14(config.restaurant_test, name='Restaurant')\n",
    "election_train = parsers.election_train(config.ELECTION, name='Election Train')\n",
    "election_test = parsers.election_test(config.ELECTION, name='Election')\n",
    "mitchell_train = parsers.semeval_14(config.mitchell_train, name='Mitchell Train')\n",
    "mitchell_test = parsers.semeval_14(config.mitchell_test, name='Mitchell')\n",
    "youtubean_train = parsers.semeval_14(config.youtubean_train, name='YouTuBean Train')\n",
    "youtubean_test = parsers.semeval_14(config.youtubean_test, name='YouTuBean')\n",
    "train_datasets = [dong_train, laptop_train, restaurant_train, election_train, mitchell_train, youtubean_train]\n",
    "test_datasets = [dong_test, laptop_test, restaurant_test, election_test, mitchell_test, youtubean_test]\n",
    "all_datasets = train_datasets + test_datasets\n",
    "\n",
    "all_datasets_lower_words = [dataset.word_list(tokeniser, lower=True) \n",
    "                            for dataset in all_datasets]\n",
    "all_datasets_lower_words = [word for words in all_datasets_lower_words \n",
    "                                 for word in words]\n",
    "default_word_vector = GloveCommonCrawl(version=840, \n",
    "                                       filter_words=all_datasets_lower_words)\n",
    "\n",
    "# Models\n",
    "models = [TargetDep, TargetDepPlus, TDParse, TDParsePlus]\n",
    "# Range of possible C values\n",
    "c_params = [1] # include the default sklearn C Value\n",
    "for c in range(-15, 3, 2):\n",
    "    c_params.append(math.pow(2, c))\n",
    "# Load data to find best C values\n",
    "best_c_index = load_C_cross_val_predictions(test_datasets, models)\n",
    "# Load best C for each dataset for each model\n",
    "best_c_dataset_model = get_specific_params([best_c_index], ['C'], [c_params])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the interested reader the best C values for the Macro F1 metric are shown below. However only the C parameters used will be those that performed best on the Accuracy metric. The C value is not used within the LSTM based methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "                           Dong    Laptop  Restaurant  Election  Mitchell  \\\nTarget Dependent       0.007812  0.031250    0.007812   0.12500     2.000   \nTarget Dependent Plus  0.001953  0.007812    0.007812   0.03125     1.000   \nTDParse                0.007812  0.031250    0.007812   0.12500     0.125   \nTDParsePlus            0.001953  0.007812    0.007812   0.03125     2.000   \nTDParse Minus          0.031250  0.125000    0.125000   0.50000     0.125   \n\n                       YouTuBean  \nTarget Dependent             1.0  \nTarget Dependent Plus        1.0  \nTDParse                      2.0  \nTDParsePlus                  2.0  \nTDParse Minus                2.0  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Dong</th>\n      <th>Laptop</th>\n      <th>Restaurant</th>\n      <th>Election</th>\n      <th>Mitchell</th>\n      <th>YouTuBean</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>Target Dependent</th>\n      <td>0.007812</td>\n      <td>0.031250</td>\n      <td>0.007812</td>\n      <td>0.12500</td>\n      <td>2.000</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>Target Dependent Plus</th>\n      <td>0.001953</td>\n      <td>0.007812</td>\n      <td>0.007812</td>\n      <td>0.03125</td>\n      <td>1.000</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>TDParse</th>\n      <td>0.007812</td>\n      <td>0.031250</td>\n      <td>0.007812</td>\n      <td>0.12500</td>\n      <td>0.125</td>\n      <td>2.0</td>\n    </tr>\n    <tr>\n      <th>TDParsePlus</th>\n      <td>0.001953</td>\n      <td>0.007812</td>\n      <td>0.007812</td>\n      <td>0.03125</td>\n      <td>2.000</td>\n      <td>2.0</td>\n    </tr>\n    <tr>\n      <th>TDParse Minus</th>\n      <td>0.031250</td>\n      <td>0.125000</td>\n      <td>0.125000</td>\n      <td>0.50000</td>\n      <td>0.125</td>\n      <td>2.0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 4
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "macro_best_c_index = load_C_cross_val_predictions(test_datasets, models + [TDParseMinus], 'macro')\n",
    "# Load best C for each dataset for each model\n",
    "macro_best_c_dataset_model = get_specific_params([macro_best_c_index], ['C'], [c_params])\n",
    "temp_macro_best_c_dataset_model = {}\n",
    "for d_name, m_name_value in macro_best_c_dataset_model.items():\n",
    "    d_name_dict = {}\n",
    "    for m_name, value in m_name_value.items():\n",
    "        d_name_dict[m_name] = value['C']\n",
    "    temp_macro_best_c_dataset_model[d_name] = d_name_dict\n",
    "pd.DataFrame(temp_macro_best_c_dataset_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The best C values for the accuracy metric:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "                           Dong    Laptop  Restaurant  Election  Mitchell  \\\nTarget Dependent       0.007812  0.007812    0.007812  0.007812  0.007812   \nTarget Dependent Plus  0.001953  0.007812    0.007812  0.031250  0.001953   \nTDParse                0.007812  0.007812    0.007812  0.007812  0.007812   \nTDParsePlus            0.001953  0.007812    0.007812  0.007812  0.007812   \nTDParse Minus          0.007812  0.125000    0.031250  0.125000  0.031250   \n\n                       YouTuBean  \nTarget Dependent        0.031250  \nTarget Dependent Plus   0.031250  \nTDParse                 2.000000  \nTDParsePlus             0.007812  \nTDParse Minus           0.031250  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Dong</th>\n      <th>Laptop</th>\n      <th>Restaurant</th>\n      <th>Election</th>\n      <th>Mitchell</th>\n      <th>YouTuBean</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>Target Dependent</th>\n      <td>0.007812</td>\n      <td>0.007812</td>\n      <td>0.007812</td>\n      <td>0.007812</td>\n      <td>0.007812</td>\n      <td>0.031250</td>\n    </tr>\n    <tr>\n      <th>Target Dependent Plus</th>\n      <td>0.001953</td>\n      <td>0.007812</td>\n      <td>0.007812</td>\n      <td>0.031250</td>\n      <td>0.001953</td>\n      <td>0.031250</td>\n    </tr>\n    <tr>\n      <th>TDParse</th>\n      <td>0.007812</td>\n      <td>0.007812</td>\n      <td>0.007812</td>\n      <td>0.007812</td>\n      <td>0.007812</td>\n      <td>2.000000</td>\n    </tr>\n    <tr>\n      <th>TDParsePlus</th>\n      <td>0.001953</td>\n      <td>0.007812</td>\n      <td>0.007812</td>\n      <td>0.007812</td>\n      <td>0.007812</td>\n      <td>0.007812</td>\n    </tr>\n    <tr>\n      <th>TDParse Minus</th>\n      <td>0.007812</td>\n      <td>0.125000</td>\n      <td>0.031250</td>\n      <td>0.125000</td>\n      <td>0.031250</td>\n      <td>0.031250</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 5
    }
   ],
   "source": [
    "temp_best_c_index = load_C_cross_val_predictions(test_datasets, models + [TDParseMinus])\n",
    "# Load best C for each dataset for each model\n",
    "temp_best_c_dataset_model = get_specific_params([temp_best_c_index], ['C'], [c_params])\n",
    "\n",
    "temp_accuracy_best_c_dataset_model = {}\n",
    "for d_name, m_name_value in temp_best_c_dataset_model.items():\n",
    "    d_name_dict = {}\n",
    "    for m_name, value in m_name_value.items():\n",
    "        d_name_dict[m_name] = value['C']\n",
    "    temp_accuracy_best_c_dataset_model[d_name] = d_name_dict\n",
    "pd.DataFrame(temp_accuracy_best_c_dataset_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we train and evaluate the models across all six datasets and save the predictions. We also show the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "0.4578557014465332\n"
    }
   ],
   "source": [
    "def run_save_predictions(train: TargetCollection, test: TargetCollection, \n",
    "                         model: SKLearnModel, save_dir: Path,\n",
    "                         save_name_addon: str = '') -> np.ndarray:\n",
    "    '''\n",
    "    Given a training and test dataset, it will run train the model and \n",
    "    make predictions on the test datasets, of wich those results will be \n",
    "    saved. The results from the test predictions will be returned. \n",
    "    If the results already exist the model will not be re-trained \n",
    "    but rather the saved predictions will be returned (caching). \n",
    "    '''\n",
    "    dataset_name = train.name.split()[0]\n",
    "    save_dir = Path(save_dir, model.name())\n",
    "    save_dir.mkdir(parents=True, exist_ok=True)\n",
    "    if save_name_addon != '':\n",
    "        save_file = Path(save_dir, f'{save_name_addon} {dataset_name}.npy')\n",
    "    else:\n",
    "        save_file = Path(save_dir, f'{dataset_name}.npy')\n",
    "    if save_file.exists():\n",
    "        return np.load(save_file)\n",
    "    print(save_file)\n",
    "    \n",
    "    X_train = train.data_dict()\n",
    "    X_test = test.data_dict()\n",
    "    y_train = train.sentiment_data()\n",
    "    \n",
    "    model.fit(X_train, y_train)\n",
    "    raw_predictions = model.predict(X_test)\n",
    "    np.save(save_file, raw_predictions)\n",
    "    return raw_predictions\n",
    "\n",
    "#results_datasets = []\n",
    "#results_models = []\n",
    "#results_accuracy = []\n",
    "#results_f1 = []\n",
    "for train_dataset, test_dataset in zip(train_datasets, test_datasets):\n",
    "    dataset_name = test_dataset.name\n",
    "    #print(dataset_name)\n",
    "    true_values = test_dataset.sentiment_data()\n",
    "    for model in models:\n",
    "        model_name = model.name()\n",
    "        best_c = best_c_dataset_model[dataset_name][model_name]['C']\n",
    "        a_model_params = {}\n",
    "        if 'TDParse' in model_name:\n",
    "            a_model_params['parser'] = tweebo\n",
    "        if 'Plus' in model_name:\n",
    "            a_model_params['senti_lexicon'] = all_lexicons\n",
    "        a_model_params['word_vectors'] = [default_word_vector]\n",
    "        a_model_params['tokeniser'] = tokeniser\n",
    "        a_model_params['scale'] = MinMaxScaler()\n",
    "        a_model_params['C'] = best_c\n",
    "        a_model_params['lower'] = True\n",
    "        a_model_params['random_state'] = default_random_state\n",
    "        a_model = model(**a_model_params)\n",
    "        predictions = run_save_predictions(train_dataset, test_dataset, \n",
    "                                           a_model, result_dir)\n",
    "        #accuracy = accuracy_score(true_values, predictions)\n",
    "        #results_accuracy.append(accuracy)\n",
    "        #f1 = f1_score(true_values, predictions, average='macro')\n",
    "        #results_f1.append(f1)\n",
    "        #results_datasets.append(dataset_name)\n",
    "        #results_models.append(model_name)\n",
    "# Load the LSTM methods\n",
    "#dataset_names = ['YouTuBean', 'Dong', 'Election', 'Laptop', 'Mitchell', 'Restaurant']\n",
    "#for dataset_name in dataset_names:\n",
    "#    for model_name in ['LSTM', 'TDLSTM', 'TCLSTM']:\n",
    "#        lstm_result_dir = config.RESULTS_DIR / 'Mass Evaluation' / 'patience 10' / f\"{model_name}\"\n",
    "#        test_result_fp = lstm_result_dir / f'{dataset_name} test.json'\n",
    "#        with test_result_fp.open('r') as result_file:\n",
    "#            a_result = json.load(result_file)\n",
    "#            a_result = load_convert_results(a_result, label_mapper)\n",
    "#            acc_scores = get_score(a_result[0], a_result[1], accuracy_score)\n",
    "#            f1_scores = get_score(a_result[0], a_result[1], f1_score, average='macro')\n",
    "#            for i in range(len(acc_scores)):\n",
    "#                results_datasets.append(dataset_name)\n",
    "#                results_models.append(model_name)\n",
    "#                results_accuracy.append(acc_scores[i])\n",
    "#                results_f1.append(f1_scores[i])\n",
    "#results_df = {'F1': results_f1, 'Accuracy': results_accuracy, \n",
    "#              'Dataset': results_datasets, 'Model': results_models}\n",
    "#results_df = pd.DataFrame(results_df)\n",
    "#pd.pivot_table(data=results_df, values=['F1', 'Accuracy'], index='Dataset', columns='Model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the cell below creates the prediction for all datasets trained on the much smaller sized training dataset. This smaller sized training set is the same size as the YouTuBean training dataset size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Dong\nLaptop\nRestaurant\nElection\nMitchell\n0.5857939720153809\n"
    }
   ],
   "source": [
    "dong_small_train = parsers.semeval_14(config.small_training_dataset_dir / 'Dong train.xml', \n",
    "                                      name='Dong Train')\n",
    "laptop_small_train = parsers.semeval_14(config.small_training_dataset_dir / 'Laptop train.xml', \n",
    "                                        name='Laptop Train')\n",
    "restaurant_small_train = parsers.semeval_14(config.small_training_dataset_dir / 'Restaurant train.xml', \n",
    "                                            name='Restaurant Train')\n",
    "election_small_train = parsers.semeval_14(config.small_training_dataset_dir / 'Election train.xml', \n",
    "                                          name='Election Train')\n",
    "mitchell_small_train = parsers.semeval_14(config.small_training_dataset_dir / 'Mitchell train.xml', \n",
    "                                          name='Mitchell Train')\n",
    "small_training_datasets = [dong_small_train, laptop_small_train, restaurant_small_train, \n",
    "                           election_small_train, mitchell_small_train]\n",
    "small_test_datasets = [dong_test, laptop_test, restaurant_test, election_test, mitchell_test]\n",
    "\n",
    "for train_dataset, test_dataset in zip(small_training_datasets, small_test_datasets):\n",
    "    dataset_name = test_dataset.name\n",
    "    print(dataset_name)\n",
    "    true_values = test_dataset.sentiment_data()\n",
    "    for model in models:\n",
    "        model_name = model.name()\n",
    "        best_c = best_c_dataset_model[dataset_name][model_name]['C']\n",
    "        a_model_params = {}\n",
    "        if 'TDParse' in model_name:\n",
    "            a_model_params['parser'] = tweebo\n",
    "        if 'Plus' in model_name:\n",
    "            a_model_params['senti_lexicon'] = all_lexicons\n",
    "        a_model_params['word_vectors'] = [default_word_vector]\n",
    "        a_model_params['tokeniser'] = tokeniser\n",
    "        a_model_params['scale'] = MinMaxScaler()\n",
    "        a_model_params['C'] = best_c\n",
    "        a_model_params['lower'] = True\n",
    "        a_model_params['random_state'] = default_random_state\n",
    "        a_model = model(**a_model_params)\n",
    "        predictions = run_save_predictions(train_dataset, test_dataset, \n",
    "                                           a_model, small_result_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "Dataset                                 Dong      Election        Laptop  \\\n         Model                                                             \nAccuracy LSTM                   64.57 (0.99)  47.85 (0.54)  59.90 (2.94)   \n         TCLSTM                 68.98 (0.58)  57.40 (0.22)  56.77 (3.34)   \n         TDLSTM                 71.12 (0.50)  57.50 (0.80)  61.76 (0.66)   \n         TDParse                67.77 (0.00)  57.46 (0.00)  67.08 (0.00)   \n         TDParsePlus            69.36 (0.00)  56.12 (0.00)  68.50 (0.00)   \n         Target Dependent       68.50 (0.00)  57.22 (0.00)  66.14 (0.00)   \n         Target Dependent Plus  70.23 (0.00)  53.21 (0.00)  68.97 (0.00)   \n\nDataset                             Mitchell    Restaurant     YouTuBean  \n         Model                                                            \nAccuracy LSTM                   71.04 (0.60)  68.63 (3.67)  63.33 (0.00)  \n         TCLSTM                 70.77 (0.37)  71.85 (0.71)  66.81 (1.19)  \n         TDLSTM                 70.52 (0.60)  73.56 (0.47)  64.17 (0.96)  \n         TDParse                73.96 (0.00)  77.95 (0.00)  79.58 (0.00)  \n         TDParsePlus            73.35 (0.00)  78.30 (0.00)  83.33 (0.00)  \n         Target Dependent       73.45 (0.00)  77.32 (0.00)  82.50 (0.00)  \n         Target Dependent Plus  74.37 (0.00)  78.04 (0.00)  81.67 (0.00)  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Dataset</th>\n      <th>Dong</th>\n      <th>Election</th>\n      <th>Laptop</th>\n      <th>Mitchell</th>\n      <th>Restaurant</th>\n      <th>YouTuBean</th>\n    </tr>\n    <tr>\n      <th></th>\n      <th>Model</th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th rowspan=\"7\" valign=\"top\">Accuracy</th>\n      <th>LSTM</th>\n      <td>64.57 (0.99)</td>\n      <td>47.85 (0.54)</td>\n      <td>59.90 (2.94)</td>\n      <td>71.04 (0.60)</td>\n      <td>68.63 (3.67)</td>\n      <td>63.33 (0.00)</td>\n    </tr>\n    <tr>\n      <th>TCLSTM</th>\n      <td>68.98 (0.58)</td>\n      <td>57.40 (0.22)</td>\n      <td>56.77 (3.34)</td>\n      <td>70.77 (0.37)</td>\n      <td>71.85 (0.71)</td>\n      <td>66.81 (1.19)</td>\n    </tr>\n    <tr>\n      <th>TDLSTM</th>\n      <td>71.12 (0.50)</td>\n      <td>57.50 (0.80)</td>\n      <td>61.76 (0.66)</td>\n      <td>70.52 (0.60)</td>\n      <td>73.56 (0.47)</td>\n      <td>64.17 (0.96)</td>\n    </tr>\n    <tr>\n      <th>TDParse</th>\n      <td>67.77 (0.00)</td>\n      <td>57.46 (0.00)</td>\n      <td>67.08 (0.00)</td>\n      <td>73.96 (0.00)</td>\n      <td>77.95 (0.00)</td>\n      <td>79.58 (0.00)</td>\n    </tr>\n    <tr>\n      <th>TDParsePlus</th>\n      <td>69.36 (0.00)</td>\n      <td>56.12 (0.00)</td>\n      <td>68.50 (0.00)</td>\n      <td>73.35 (0.00)</td>\n      <td>78.30 (0.00)</td>\n      <td>83.33 (0.00)</td>\n    </tr>\n    <tr>\n      <th>Target Dependent</th>\n      <td>68.50 (0.00)</td>\n      <td>57.22 (0.00)</td>\n      <td>66.14 (0.00)</td>\n      <td>73.45 (0.00)</td>\n      <td>77.32 (0.00)</td>\n      <td>82.50 (0.00)</td>\n    </tr>\n    <tr>\n      <th>Target Dependent Plus</th>\n      <td>70.23 (0.00)</td>\n      <td>53.21 (0.00)</td>\n      <td>68.97 (0.00)</td>\n      <td>74.37 (0.00)</td>\n      <td>78.04 (0.00)</td>\n      <td>81.67 (0.00)</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 38
    }
   ],
   "source": [
    "def mean_std(data: pd.Series) -> str:\n",
    "   to_percentage = data * 100\n",
    "   return f'{np.mean(to_percentage):.2f} ({np.std(to_percentage):.2f})'\n",
    "pd.pivot_table(data=results_df, values=['Accuracy'], index='Dataset', columns='Model', aggfunc=mean_std).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "Dataset                                 Dong      Election        Laptop  \\\n         Model                                                             \nAccuracy LSTM                   60.79 (4.99)  47.84 (0.32)  53.19 (0.61)   \n         TCLSTM                 66.71 (2.87)  56.34 (0.55)  52.30 (0.75)   \n         TDLSTM                 69.77 (1.67)  56.31 (0.56)  54.36 (0.57)   \n         TDParse                67.77 (0.00)  57.46 (0.00)  67.08 (0.00)   \n         TDParsePlus            69.36 (0.00)  56.12 (0.00)  68.50 (0.00)   \n         Target Dependent       68.50 (0.00)  57.22 (0.00)  66.14 (0.00)   \n         Target Dependent Plus  70.23 (0.00)  53.21 (0.00)  68.97 (0.00)   \n\nDataset                             Mitchell    Restaurant     YouTuBean  \n         Model                                                            \nAccuracy LSTM                   70.13 (0.04)  66.10 (2.46)  63.33 (0.00)  \n         TCLSTM                 70.21 (0.14)  66.65 (2.08)  63.19 (0.39)  \n         TDLSTM                 70.11 (0.00)  68.04 (3.06)  63.33 (0.00)  \n         TDParse                73.96 (0.00)  77.95 (0.00)  79.58 (0.00)  \n         TDParsePlus            73.35 (0.00)  78.30 (0.00)  83.33 (0.00)  \n         Target Dependent       73.45 (0.00)  77.32 (0.00)  82.50 (0.00)  \n         Target Dependent Plus  74.37 (0.00)  78.04 (0.00)  81.67 (0.00)  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Dataset</th>\n      <th>Dong</th>\n      <th>Election</th>\n      <th>Laptop</th>\n      <th>Mitchell</th>\n      <th>Restaurant</th>\n      <th>YouTuBean</th>\n    </tr>\n    <tr>\n      <th></th>\n      <th>Model</th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th rowspan=\"7\" valign=\"top\">Accuracy</th>\n      <th>LSTM</th>\n      <td>60.79 (4.99)</td>\n      <td>47.84 (0.32)</td>\n      <td>53.19 (0.61)</td>\n      <td>70.13 (0.04)</td>\n      <td>66.10 (2.46)</td>\n      <td>63.33 (0.00)</td>\n    </tr>\n    <tr>\n      <th>TCLSTM</th>\n      <td>66.71 (2.87)</td>\n      <td>56.34 (0.55)</td>\n      <td>52.30 (0.75)</td>\n      <td>70.21 (0.14)</td>\n      <td>66.65 (2.08)</td>\n      <td>63.19 (0.39)</td>\n    </tr>\n    <tr>\n      <th>TDLSTM</th>\n      <td>69.77 (1.67)</td>\n      <td>56.31 (0.56)</td>\n      <td>54.36 (0.57)</td>\n      <td>70.11 (0.00)</td>\n      <td>68.04 (3.06)</td>\n      <td>63.33 (0.00)</td>\n    </tr>\n    <tr>\n      <th>TDParse</th>\n      <td>67.77 (0.00)</td>\n      <td>57.46 (0.00)</td>\n      <td>67.08 (0.00)</td>\n      <td>73.96 (0.00)</td>\n      <td>77.95 (0.00)</td>\n      <td>79.58 (0.00)</td>\n    </tr>\n    <tr>\n      <th>TDParsePlus</th>\n      <td>69.36 (0.00)</td>\n      <td>56.12 (0.00)</td>\n      <td>68.50 (0.00)</td>\n      <td>73.35 (0.00)</td>\n      <td>78.30 (0.00)</td>\n      <td>83.33 (0.00)</td>\n    </tr>\n    <tr>\n      <th>Target Dependent</th>\n      <td>68.50 (0.00)</td>\n      <td>57.22 (0.00)</td>\n      <td>66.14 (0.00)</td>\n      <td>73.45 (0.00)</td>\n      <td>77.32 (0.00)</td>\n      <td>82.50 (0.00)</td>\n    </tr>\n    <tr>\n      <th>Target Dependent Plus</th>\n      <td>70.23 (0.00)</td>\n      <td>53.21 (0.00)</td>\n      <td>68.97 (0.00)</td>\n      <td>74.37 (0.00)</td>\n      <td>78.04 (0.00)</td>\n      <td>81.67 (0.00)</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 34
    }
   ],
   "source": [
    "def mean_std(data: pd.Series) -> str:\n",
    "   to_percentage = data * 100\n",
    "   return f'{np.mean(to_percentage):.2f} ({np.std(to_percentage):.2f})'\n",
    "pd.pivot_table(data=results_df, values=['Accuracy'], index='Dataset', columns='Model', aggfunc=mean_std).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "Dataset                           Dong      Election        Laptop  \\\n   Model                                                             \nF1 LSTM                   61.58 (1.29)  30.65 (1.09)  41.91 (5.54)   \n   TCLSTM                 65.92 (0.97)  43.57 (0.71)  45.04 (5.15)   \n   TDLSTM                 68.54 (0.60)  42.54 (2.25)  49.82 (1.61)   \n   TDParse                64.33 (0.00)  46.64 (0.00)  59.23 (0.00)   \n   TDParsePlus            66.36 (0.00)  46.30 (0.00)  61.89 (0.00)   \n   Target Dependent       65.27 (0.00)  46.60 (0.00)  57.86 (0.00)   \n   Target Dependent Plus  67.36 (0.00)  44.52 (0.00)  62.33 (0.00)   \n\nDataset                       Mitchell     Restaurant     YouTuBean  \n   Model                                                             \nF1 LSTM                   35.51 (4.16)  37.15 (10.98)  25.85 (0.00)  \n   TCLSTM                 38.42 (6.70)   53.54 (1.79)  36.83 (3.05)  \n   TDLSTM                 29.67 (3.30)   56.66 (1.13)  28.98 (3.05)  \n   TDParse                48.58 (0.00)   64.66 (0.00)  70.05 (0.00)  \n   TDParsePlus            51.17 (0.00)   65.26 (0.00)  74.46 (0.00)  \n   Target Dependent       48.98 (0.00)   63.17 (0.00)  74.80 (0.00)  \n   Target Dependent Plus  48.08 (0.00)   64.44 (0.00)  72.90 (0.00)  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Dataset</th>\n      <th>Dong</th>\n      <th>Election</th>\n      <th>Laptop</th>\n      <th>Mitchell</th>\n      <th>Restaurant</th>\n      <th>YouTuBean</th>\n    </tr>\n    <tr>\n      <th></th>\n      <th>Model</th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th rowspan=\"7\" valign=\"top\">F1</th>\n      <th>LSTM</th>\n      <td>61.58 (1.29)</td>\n      <td>30.65 (1.09)</td>\n      <td>41.91 (5.54)</td>\n      <td>35.51 (4.16)</td>\n      <td>37.15 (10.98)</td>\n      <td>25.85 (0.00)</td>\n    </tr>\n    <tr>\n      <th>TCLSTM</th>\n      <td>65.92 (0.97)</td>\n      <td>43.57 (0.71)</td>\n      <td>45.04 (5.15)</td>\n      <td>38.42 (6.70)</td>\n      <td>53.54 (1.79)</td>\n      <td>36.83 (3.05)</td>\n    </tr>\n    <tr>\n      <th>TDLSTM</th>\n      <td>68.54 (0.60)</td>\n      <td>42.54 (2.25)</td>\n      <td>49.82 (1.61)</td>\n      <td>29.67 (3.30)</td>\n      <td>56.66 (1.13)</td>\n      <td>28.98 (3.05)</td>\n    </tr>\n    <tr>\n      <th>TDParse</th>\n      <td>64.33 (0.00)</td>\n      <td>46.64 (0.00)</td>\n      <td>59.23 (0.00)</td>\n      <td>48.58 (0.00)</td>\n      <td>64.66 (0.00)</td>\n      <td>70.05 (0.00)</td>\n    </tr>\n    <tr>\n      <th>TDParsePlus</th>\n      <td>66.36 (0.00)</td>\n      <td>46.30 (0.00)</td>\n      <td>61.89 (0.00)</td>\n      <td>51.17 (0.00)</td>\n      <td>65.26 (0.00)</td>\n      <td>74.46 (0.00)</td>\n    </tr>\n    <tr>\n      <th>Target Dependent</th>\n      <td>65.27 (0.00)</td>\n      <td>46.60 (0.00)</td>\n      <td>57.86 (0.00)</td>\n      <td>48.98 (0.00)</td>\n      <td>63.17 (0.00)</td>\n      <td>74.80 (0.00)</td>\n    </tr>\n    <tr>\n      <th>Target Dependent Plus</th>\n      <td>67.36 (0.00)</td>\n      <td>44.52 (0.00)</td>\n      <td>62.33 (0.00)</td>\n      <td>48.08 (0.00)</td>\n      <td>64.44 (0.00)</td>\n      <td>72.90 (0.00)</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 39
    }
   ],
   "source": [
    "pd.pivot_table(data=results_df, values=['F1'], index='Dataset', columns='Model', aggfunc=mean_std).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "Dataset                            Dong      Election        Laptop  \\\n   Model                                                              \nF1 LSTM                   52.99 (13.04)  28.97 (1.81)  26.03 (4.08)   \n   TCLSTM                  62.87 (4.34)  40.25 (0.84)  37.17 (1.00)   \n   TDLSTM                  66.71 (2.23)  39.93 (0.63)  37.46 (1.00)   \n   TDParse                 64.33 (0.00)  46.64 (0.00)  59.23 (0.00)   \n   TDParsePlus             66.36 (0.00)  46.30 (0.00)  61.89 (0.00)   \n   Target Dependent        65.27 (0.00)  46.60 (0.00)  57.86 (0.00)   \n   Target Dependent Plus   67.36 (0.00)  44.52 (0.00)  62.33 (0.00)   \n\nDataset                       Mitchell     Restaurant     YouTuBean  \n   Model                                                             \nF1 LSTM                   27.63 (0.35)   29.65 (7.57)  25.85 (0.00)  \n   TCLSTM                 27.80 (0.46)   34.44 (9.09)  27.43 (1.23)  \n   TDLSTM                 27.48 (0.00)  38.17 (11.10)  25.85 (0.00)  \n   TDParse                48.58 (0.00)   64.66 (0.00)  70.05 (0.00)  \n   TDParsePlus            51.17 (0.00)   65.26 (0.00)  74.46 (0.00)  \n   Target Dependent       48.98 (0.00)   63.17 (0.00)  74.80 (0.00)  \n   Target Dependent Plus  48.08 (0.00)   64.44 (0.00)  72.90 (0.00)  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Dataset</th>\n      <th>Dong</th>\n      <th>Election</th>\n      <th>Laptop</th>\n      <th>Mitchell</th>\n      <th>Restaurant</th>\n      <th>YouTuBean</th>\n    </tr>\n    <tr>\n      <th></th>\n      <th>Model</th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th rowspan=\"7\" valign=\"top\">F1</th>\n      <th>LSTM</th>\n      <td>52.99 (13.04)</td>\n      <td>28.97 (1.81)</td>\n      <td>26.03 (4.08)</td>\n      <td>27.63 (0.35)</td>\n      <td>29.65 (7.57)</td>\n      <td>25.85 (0.00)</td>\n    </tr>\n    <tr>\n      <th>TCLSTM</th>\n      <td>62.87 (4.34)</td>\n      <td>40.25 (0.84)</td>\n      <td>37.17 (1.00)</td>\n      <td>27.80 (0.46)</td>\n      <td>34.44 (9.09)</td>\n      <td>27.43 (1.23)</td>\n    </tr>\n    <tr>\n      <th>TDLSTM</th>\n      <td>66.71 (2.23)</td>\n      <td>39.93 (0.63)</td>\n      <td>37.46 (1.00)</td>\n      <td>27.48 (0.00)</td>\n      <td>38.17 (11.10)</td>\n      <td>25.85 (0.00)</td>\n    </tr>\n    <tr>\n      <th>TDParse</th>\n      <td>64.33 (0.00)</td>\n      <td>46.64 (0.00)</td>\n      <td>59.23 (0.00)</td>\n      <td>48.58 (0.00)</td>\n      <td>64.66 (0.00)</td>\n      <td>70.05 (0.00)</td>\n    </tr>\n    <tr>\n      <th>TDParsePlus</th>\n      <td>66.36 (0.00)</td>\n      <td>46.30 (0.00)</td>\n      <td>61.89 (0.00)</td>\n      <td>51.17 (0.00)</td>\n      <td>65.26 (0.00)</td>\n      <td>74.46 (0.00)</td>\n    </tr>\n    <tr>\n      <th>Target Dependent</th>\n      <td>65.27 (0.00)</td>\n      <td>46.60 (0.00)</td>\n      <td>57.86 (0.00)</td>\n      <td>48.98 (0.00)</td>\n      <td>63.17 (0.00)</td>\n      <td>74.80 (0.00)</td>\n    </tr>\n    <tr>\n      <th>Target Dependent Plus</th>\n      <td>67.36 (0.00)</td>\n      <td>44.52 (0.00)</td>\n      <td>62.33 (0.00)</td>\n      <td>48.08 (0.00)</td>\n      <td>64.44 (0.00)</td>\n      <td>72.90 (0.00)</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 31
    }
   ],
   "source": [
    "pd.pivot_table(data=results_df, values=['F1'], index='Dataset', columns='Model', aggfunc=mean_std).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we show the C-Values that were used for each method for each dataset. This can be seen in [./large_scale_feature_settings.ipynb](./large_scale_feature_settings.ipynb) notebook through the graphs, but the table below is easier to read:  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "Model        TDParse  TDParsePlus  Target Dependent  Target Dependent Plus\nDataset                                                                   \nDong        0.007812     0.001953          0.007812               0.001953\nElection    0.007812     0.007812          0.007812               0.031250\nLaptop      0.007812     0.007812          0.007812               0.007812\nMitchell    0.007812     0.007812          0.007812               0.001953\nRestaurant  0.007812     0.007812          0.007812               0.007812\nYouTuBean   2.000000     0.007812          0.031250               0.031250",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th>Model</th>\n      <th>TDParse</th>\n      <th>TDParsePlus</th>\n      <th>Target Dependent</th>\n      <th>Target Dependent Plus</th>\n    </tr>\n    <tr>\n      <th>Dataset</th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>Dong</th>\n      <td>0.007812</td>\n      <td>0.001953</td>\n      <td>0.007812</td>\n      <td>0.001953</td>\n    </tr>\n    <tr>\n      <th>Election</th>\n      <td>0.007812</td>\n      <td>0.007812</td>\n      <td>0.007812</td>\n      <td>0.031250</td>\n    </tr>\n    <tr>\n      <th>Laptop</th>\n      <td>0.007812</td>\n      <td>0.007812</td>\n      <td>0.007812</td>\n      <td>0.007812</td>\n    </tr>\n    <tr>\n      <th>Mitchell</th>\n      <td>0.007812</td>\n      <td>0.007812</td>\n      <td>0.007812</td>\n      <td>0.001953</td>\n    </tr>\n    <tr>\n      <th>Restaurant</th>\n      <td>0.007812</td>\n      <td>0.007812</td>\n      <td>0.007812</td>\n      <td>0.007812</td>\n    </tr>\n    <tr>\n      <th>YouTuBean</th>\n      <td>2.000000</td>\n      <td>0.007812</td>\n      <td>0.031250</td>\n      <td>0.031250</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 12
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "c_dataset_names = []\n",
    "c_model_names = []\n",
    "c_model_dataset_values = []\n",
    "for c_dataset_name, model_c_value in best_c_dataset_model.items():\n",
    "    for model_c, values in model_c_value.items():\n",
    "        value = values['C']\n",
    "        c_dataset_names.append(c_dataset_name)\n",
    "        c_model_names.append(model_c)\n",
    "        c_model_dataset_values.append(value)\n",
    "c_values_df = {'Dataset': c_dataset_names, 'Model': c_model_names, \n",
    "               'C': c_model_dataset_values}\n",
    "c_values_df = pd.DataFrame(c_values_df)\n",
    "pd.pivot_table(data=c_values_df, values='C', index='Dataset', columns='Model')"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7-candidate"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python36764bitf868c7e0d1e547669dece7a4caca1993",
   "display_name": "Python 3.6.7 64-bit"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}